# Use-parallel-techniques-to-improve-performance-of-application-of-neural-network-on-edge-devices-
Introduction As researches on the topic of Deep Learning(DL) become mature, these neural networks are getting much heavier than past. That is to say, lots of computing power is required, including GPU and some parallel algorithm behind it, like distributing training. However, if we take power consumption or GPU utilization into consideration, in most case, many models waste a lot of resources. Thus, how to reach the maximum utilization of hardware accelerators while minimizing waste of energy becomes a vital issue. The central concept behind it is energy efficiency and hardware-aware deployment strategy. Based on these stuffs to extend, it is not only for training but also quite important for inference too. If the model is energy-efficient across platforms, then we could expect that it might have a great performance(latency) on the platforms which are power limited. Therefore, the objective we want to achieve is to use some parallel techniques to efficiently deploy models to a platform, which is not that powerful like server-class workstation, or be equipped with high performance accelerator like Titan, while still keeping its performance. 
